{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHTQYU5cjYlh",
        "outputId": "630b4d1d-3e57-41d4-e388-5d39b3f08e4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_from_gdrive(data_dir):\n",
        "  # Initialize an empty list to store the DataFrames\n",
        "  data_frames = []\n",
        "\n",
        "  # Loop through each .csv file in the directory\n",
        "  for dir in data_dir:\n",
        "    for file_name in os.listdir(dir):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(dir, file_name)\n",
        "\n",
        "            # Load the .csv file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Remove the first column\n",
        "            df = df.iloc[:, 4:]\n",
        "\n",
        "            df  = df.div(360)\n",
        "\n",
        "            df  = df.clip(upper =1)\n",
        "\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            data_frames.append(df)\n",
        "\n",
        "  return data_frames"
      ],
      "metadata": {
        "id": "ofEnBcohjdb9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the directory path where the .csv files are located in Google Drive\n",
        "data_dir = ['/content/drive/MyDrive/28 08 mod/Normal']\n",
        "\n",
        "data_frames =  load_from_gdrive(data_dir)\n"
      ],
      "metadata": {
        "id": "6Tpf7rjCC3cc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "0ITgncBJjEzz",
        "outputId": "fc2ac9ee-5814-4339-f0f0-b9da4c5cf2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Reconstruction Loss: 4.1486, KL Divergence Loss: 0.0559, Total Loss: 4.2045\n",
            "Epoch 2/100, Reconstruction Loss: 3.6640, KL Divergence Loss: 0.0606, Total Loss: 3.7246\n",
            "Epoch 3/100, Reconstruction Loss: 3.2720, KL Divergence Loss: 0.0629, Total Loss: 3.3349\n",
            "Epoch 4/100, Reconstruction Loss: 2.8841, KL Divergence Loss: 0.0560, Total Loss: 2.9401\n",
            "Epoch 5/100, Reconstruction Loss: 2.6155, KL Divergence Loss: 0.0569, Total Loss: 2.6723\n",
            "Epoch 6/100, Reconstruction Loss: 2.3900, KL Divergence Loss: 0.0539, Total Loss: 2.4439\n",
            "Epoch 7/100, Reconstruction Loss: 2.2901, KL Divergence Loss: 0.0497, Total Loss: 2.3398\n",
            "Epoch 8/100, Reconstruction Loss: 2.0327, KL Divergence Loss: 0.0452, Total Loss: 2.0779\n",
            "Epoch 9/100, Reconstruction Loss: 1.9360, KL Divergence Loss: 0.0331, Total Loss: 1.9692\n",
            "Epoch 10/100, Reconstruction Loss: 1.7798, KL Divergence Loss: 0.0259, Total Loss: 1.8057\n",
            "Epoch 11/100, Reconstruction Loss: 1.6807, KL Divergence Loss: 0.0229, Total Loss: 1.7035\n",
            "Epoch 12/100, Reconstruction Loss: 1.6032, KL Divergence Loss: 0.0170, Total Loss: 1.6202\n",
            "Epoch 13/100, Reconstruction Loss: 1.5869, KL Divergence Loss: 0.0149, Total Loss: 1.6018\n",
            "Epoch 14/100, Reconstruction Loss: 1.4462, KL Divergence Loss: 0.0126, Total Loss: 1.4588\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ab0f555028b3>\u001b[0m in \u001b[0;36m<cell line: 156>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ab0f555028b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, optimizer, timestep, epochs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0msub_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ab0f555028b3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ab0f555028b3>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    880\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Determine the maximum length to establish the maximum timestep\n",
        "max_length = max(len(df) for df in data_frames)\n",
        "\n",
        "# Iterate over each data frame and apply truncation if necessary\n",
        "timestep = 10  # Set timestep size\n",
        "truncated_data_frames = []\n",
        "truncated_data_frames = []\n",
        "for df in data_frames:\n",
        "    length = len(df)\n",
        "    if length % timestep != 0:\n",
        "        truncated_length = (length // timestep) * timestep  # Truncate to the nearest multiple of timestep\n",
        "        truncated_df = df[:truncated_length]\n",
        "        truncated_data_frames.append(truncated_df)\n",
        "    else:\n",
        "        truncated_data_frames.append(df)\n",
        "\n",
        "train_data, test_data = train_test_split(truncated_data_frames, test_size=0.4)# 60 40\n",
        "\n",
        "# Concatenate preprocessed data frames into a single tensor\n",
        "train_sequences = [torch.Tensor(df.values) for df in train_data]\n",
        "test_sequences = [torch.Tensor(df.values) for df in test_data]\n",
        "# Function to make the buckets\n",
        "def create_buckets(sequences, bucket_size):\n",
        "    sequences.sort(key=len)\n",
        "    buckets = []\n",
        "    current_bucket = []\n",
        "    max_length = 0\n",
        "    for seq in sequences:\n",
        "        if len(seq) > max_length + bucket_size:\n",
        "            if current_bucket:\n",
        "                buckets.append(current_bucket)\n",
        "            current_bucket = []\n",
        "            max_length = len(seq)\n",
        "        current_bucket.append(seq)\n",
        "    if current_bucket:\n",
        "        buckets.append(current_bucket)\n",
        "    return buckets\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch.sort(key=len, reverse=True)\n",
        "    batch = pad_sequence(batch, batch_first=True)\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Create buckets\n",
        "bucket_size = 25  # Set bucket size (25)\n",
        "train_buckets = create_buckets(train_sequences, bucket_size)\n",
        "test_buckets = create_buckets(test_sequences, bucket_size)\n",
        "\n",
        "\n",
        "batch_size = 1  # Only 1\n",
        "\n",
        "# Create a DataLoader for each bucket\n",
        "train_loaders = [DataLoader(bucket, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) for bucket in train_buckets]\n",
        "test_loaders = [DataLoader(bucket, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) for bucket in test_buckets]\n",
        "\n",
        "# Define the VAE LSTM network architecture\n",
        "class VAE_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size,dropout_rate):\n",
        "        super(VAE_LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True,dropout = dropout_rate)\n",
        "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
        "        self.fc_log_var = nn.Linear(hidden_size, latent_size)\n",
        "        self.decoder = nn.LSTM(latent_size, hidden_size, batch_first=True,dropout = dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, (h_n, _) = self.encoder(x)\n",
        "        h_n = h_n.squeeze()\n",
        "        mu = self.fc_mu(h_n)\n",
        "        log_var = self.fc_log_var(h_n)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = z.unsqueeze(0)\n",
        "        h, _ = self.decoder(z)\n",
        "        x_recon = self.fc(h.squeeze(0))\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, log_var\n",
        "\n",
        "\n",
        "input_size = 59  # Number of features in a data frame\n",
        "hidden_size = 256  #  hidden size\n",
        "latent_size = 64  #  latent size\n",
        "\n",
        "model = VAE_LSTM(input_size, hidden_size, latent_size,0.2)\n",
        "\n",
        "# training loop\n",
        "def train(model, loaders, optimizer, timestep, epochs=100):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    reconstruction_loss = nn.MSELoss(reduction='sum')\n",
        "    kl_divergence_loss = lambda mu, log_var: -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_reconstruction_loss = 0\n",
        "        total_kl_divergence_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for loader in loaders:\n",
        "            for batch in loader:\n",
        "                x = batch.to(device)\n",
        "                for i in range(0, len(x), timestep):\n",
        "                    sub_sequence = x[:, i:i+timestep, :]\n",
        "                    x_recon, mu, log_var = model(sub_sequence)\n",
        "\n",
        "                    recon_loss = reconstruction_loss(x_recon, sub_sequence)\n",
        "                    kl_loss = kl_divergence_loss(mu, log_var)\n",
        "                    loss = recon_loss + kl_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_reconstruction_loss += recon_loss.item()\n",
        "                    total_kl_divergence_loss += kl_loss.item()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "        avg_reconstruction_loss = total_reconstruction_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "        avg_kl_divergence_loss = total_kl_divergence_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "        avg_loss = total_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Reconstruction Loss: {avg_reconstruction_loss:.4f}, \"\n",
        "              f\"KL Divergence Loss: {avg_kl_divergence_loss:.4f}, \"\n",
        "              f\"Total Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "train(model, train_loaders, optimizer, timestep, epochs=100)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'trained_model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VAE_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size,dropout_rate):\n",
        "        super(VAE_LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True,dropout = dropout_rate)\n",
        "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
        "        self.fc_log_var = nn.Linear(hidden_size, latent_size)\n",
        "        self.decoder = nn.LSTM(latent_size, hidden_size, batch_first=True,dropout = dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, (h_n, _) = self.encoder(x)\n",
        "        h_n = h_n.squeeze()\n",
        "        mu = self.fc_mu(h_n)\n",
        "        log_var = self.fc_log_var(h_n)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = z.unsqueeze(0)\n",
        "        h, _ = self.decoder(z)\n",
        "        x_recon = self.fc(h.squeeze(0))\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, log_var\n",
        "\n",
        "\n",
        "def train_with_early_stopping(model, train_loaders, val_loaders, optimizer, timestep, epochs, patience, beta):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "\n",
        "    reconstruction_loss = nn.MSELoss(reduction='sum')\n",
        "    kl_divergence_loss = lambda mu, log_var: -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_reconstruction_loss = 0\n",
        "        total_kl_divergence_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for loader in train_loaders:\n",
        "            for batch in loader:\n",
        "                x = batch.to(device)\n",
        "                for i in range(0, len(x), timestep):\n",
        "                    sub_sequence = x[:, i:i+timestep, :]\n",
        "                    x_recon, mu, log_var = model(sub_sequence)\n",
        "\n",
        "                    recon_loss = reconstruction_loss(x_recon, sub_sequence)\n",
        "                    kl_loss = kl_divergence_loss(mu, log_var)\n",
        "                    loss = recon_loss + beta * kl_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_reconstruction_loss += recon_loss.item()\n",
        "                    total_kl_divergence_loss += kl_loss.item()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "        avg_reconstruction_loss = total_reconstruction_loss / sum(len(loader) for loader in val_loaders)\n",
        "        avg_kl_divergence_loss = total_kl_divergence_loss / sum(len(loader) for loader in val_loaders)\n",
        "        avg_loss = total_loss / sum(len(loader) for loader in val_loaders)\n",
        "\n",
        "\n",
        "        val_loss = evaluate(model, val_loaders, timestep, beta)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Reconstruction Loss: {avg_reconstruction_loss:.4f}, \"\n",
        "              f\"Train KL Divergence Loss: {avg_kl_divergence_loss:.4f}, \"\n",
        "              f\"Train Total Loss: {avg_loss:.4f}, \"\n",
        "              f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            #save_model(model, best_val_loss)\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "def evaluate(model, val_loaders, timestep, beta):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    total_reconstruction_loss = 0\n",
        "    total_kl_divergence_loss = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    reconstruction_loss = nn.MSELoss(reduction='sum')\n",
        "    kl_divergence_loss = lambda mu, log_var: -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for loader in val_loaders:\n",
        "            for batch in loader:\n",
        "                x = batch.to(device)\n",
        "                for i in range(0, len(x), timestep):\n",
        "                    sub_sequence = x[:, i:i+timestep, :]\n",
        "                    x_recon, mu, log_var = model(sub_sequence)\n",
        "\n",
        "                    recon_loss = reconstruction_loss(x_recon, sub_sequence)\n",
        "                    kl_loss = kl_divergence_loss(mu, log_var)\n",
        "                    loss = recon_loss + beta * kl_loss\n",
        "\n",
        "                    total_reconstruction_loss += recon_loss.item()\n",
        "                    total_kl_divergence_loss += kl_loss.item()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "    avg_reconstruction_loss = total_reconstruction_loss / sum(len(loader) for loader in val_loaders)\n",
        "    avg_kl_divergence_loss = total_kl_divergence_loss / sum(len(loader) for loader in val_loaders)\n",
        "    avg_loss = total_loss / sum(len(loader) for loader in val_loaders)\n",
        "\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_with_early_stopping(model, train_loaders, val_loaders, optimizer, timestep, epochs, patience, beta):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "\n",
        "    reconstruction_loss = nn.MSELoss(reduction='sum')\n",
        "    kl_divergence_loss = lambda mu, log_var: -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_reconstruction_loss = 0\n",
        "        total_kl_divergence_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for loader in train_loaders:\n",
        "            for batch in loader:\n",
        "                x = batch.to(device)\n",
        "                for i in range(0, len(x), timestep):\n",
        "                    sub_sequence = x[:, i:i+timestep, :]\n",
        "                    x_recon, mu, log_var = model(sub_sequence)\n",
        "\n",
        "                    recon_loss = reconstruction_loss(x_recon, sub_sequence)\n",
        "                    kl_loss = kl_divergence_loss(mu, log_var)\n",
        "                    loss = recon_loss + beta * kl_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_reconstruction_loss += recon_loss.item()\n",
        "                    total_kl_divergence_loss += kl_loss.item()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "        avg_reconstruction_loss = total_reconstruction_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "        avg_kl_divergence_loss = total_kl_divergence_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "        avg_loss = total_loss / sum(len(bucket) for bucket in train_buckets)\n",
        "\n",
        "        val_loss = evaluate(model, val_loaders, timestep, beta)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Reconstruction Loss: {avg_reconstruction_loss:.4f}, \"\n",
        "              f\"Train KL Divergence Loss: {avg_kl_divergence_loss:.4f}, \"\n",
        "              f\"Train Total Loss: {avg_loss:.4f}, \"\n",
        "              f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            #save_model(model, best_val_loss)\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "def evaluate(model, val_loaders, timestep, beta):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    total_reconstruction_loss = 0\n",
        "    total_kl_divergence_loss = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    reconstruction_loss = nn.MSELoss(reduction='sum')\n",
        "    kl_divergence_loss = lambda mu, log_var: -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for loader in val_loaders:\n",
        "            for batch in loader:\n",
        "                x = batch.to(device)\n",
        "                for i in range(0, len(x), timestep):\n",
        "                    sub_sequence = x[:, i:i+timestep, :]\n",
        "                    x_recon, mu, log_var = model(sub_sequence)\n",
        "\n",
        "                    recon_loss = reconstruction_loss(x_recon, sub_sequence)\n",
        "                    kl_loss = kl_divergence_loss(mu, log_var)\n",
        "                    loss = recon_loss + beta * kl_loss\n",
        "\n",
        "                    total_reconstruction_loss += recon_loss.item()\n",
        "                    total_kl_divergence_loss += kl_loss.item()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "    avg_reconstruction_loss = total_reconstruction_loss / sum(len(bucket) for bucket in val_buckets)\n",
        "    avg_kl_divergence_loss = total_kl_divergence_loss / sum(len(bucket) for bucket in val_buckets)\n",
        "    avg_loss = total_loss / sum(len(bucket) for bucket in val_buckets)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def save_model(model, val_loss,learning_rate,drop,hidden_size,latent_size):\n",
        "    #drive.mount('/content/drive')\n",
        "    model_name = f\"{val_loss:.4f}_hidden{hidden_size}_latent{latent_size}_timestep{timestep}_learning_rate{learning_rate}_dropout{drop}.pt\"\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/VAE_LSTM10\", model_name)\n",
        "    torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "450H0Gc6k8KF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(train_loader, val_loader, hidden_sizes, latent_sizes, learning_rates, timestep, epochs, patience,dropout):\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for latent_size in latent_sizes:\n",
        "            for learning_rate in learning_rates:\n",
        "              for drop in dropout:\n",
        "\n",
        "                  print(f'Current params: hidden_size={hidden_size}, latent_size={latent_size}, learning_rate={learning_rate}, drop = {drop}')\n",
        "                  model = VAE_LSTM(input_size, hidden_size, latent_size,dropout_rate = drop)\n",
        "                  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "                  train_with_early_stopping(model, train_loader, val_loader, optimizer, timestep, epochs, patience,beta = 1)\n",
        "                  val_loss = evaluate(model, val_loader, timestep,beta = 1)\n",
        "                  save_model(model, val_loss, learning_rate,drop,hidden_size,latent_size)\n"
      ],
      "metadata": {
        "id": "bb1O5uN1k9Cy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Determine the maximum length\n",
        "max_length = max(len(df) for df in data_frames)\n",
        "\n",
        "# Iterate over each data frame and apply truncation if necessary\n",
        "timestep = 10  # Set timestep size\n",
        "truncated_data_frames = []\n",
        "for df in data_frames:\n",
        "    length = len(df)\n",
        "    if length % timestep != 0:\n",
        "        truncated_length = (length // timestep) * timestep  # Truncate to the nearest multiple of timestep\n",
        "        truncated_df = df[:truncated_length]\n",
        "        truncated_data_frames.append(truncated_df)\n",
        "    else:\n",
        "        truncated_data_frames.append(df)\n",
        "\n",
        "# Concatenate preprocessed data frames into a single tensor\n",
        "sequences = [torch.Tensor(df.values) for df in truncated_data_frames]\n",
        "\n",
        "# Function to make the buckets\n",
        "def create_buckets(sequences, bucket_size):\n",
        "    sequences.sort(key=len)\n",
        "    buckets = []\n",
        "    current_bucket = []\n",
        "    max_length = 0\n",
        "    for seq in sequences:\n",
        "        if len(seq) > max_length + bucket_size:\n",
        "            if current_bucket:\n",
        "                buckets.append(current_bucket)\n",
        "            current_bucket = []\n",
        "            max_length = len(seq)\n",
        "        current_bucket.append(seq)\n",
        "    if current_bucket:\n",
        "        buckets.append(current_bucket)\n",
        "    return buckets\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch.sort(key=len, reverse=True)\n",
        "    batch = pad_sequence(batch, batch_first=True)\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Create buckets\n",
        "bucket_size = 25  # Set bucket size (25)\n",
        "buckets = create_buckets(sequences, bucket_size)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the buckets into training and validation sets\n",
        "train_buckets, val_buckets = train_test_split(buckets, test_size=0.4, random_state=42)\n",
        "val_buckets, test_buckets = train_test_split(val_buckets, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 1  # Only 1\n",
        "\n",
        "# Create a DataLoader for each bucket\n",
        "train_loaders = [DataLoader(bucket, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) for bucket in train_buckets]\n",
        "val_loaders = [DataLoader(bucket, batch_size=batch_size, shuffle=False, collate_fn=collate_fn) for bucket in val_buckets]\n",
        "\n",
        "hidden_sizes = [128,256, 512]\n",
        "latent_sizes = [128,256,512]\n",
        "learning_rates = [0.00001, 0.0001]\n",
        "dropout = [0,0.2,0.4]\n",
        "epochs = 100\n",
        "patience = 20\n",
        "input_size=59\n",
        "\n",
        "grid_search(train_loaders,val_loaders, hidden_sizes, latent_sizes, learning_rates,timestep, epochs, patience,dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "Q0wYbFOGk_kp",
        "outputId": "fa903458-80e2-4d82-bf41-000874383fc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current params: hidden_size=128, latent_size=128, learning_rate=1e-05, drop = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 10, 59])) that is different to the input size (torch.Size([59])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Reconstruction Loss: 7.5383, Train KL Divergence Loss: 0.3185, Train Total Loss: 7.8568, Validation Loss: 7.4266\n",
            "Epoch 2/100, Train Reconstruction Loss: 7.3733, Train KL Divergence Loss: 0.3050, Train Total Loss: 7.6782, Validation Loss: 7.4881\n",
            "Epoch 3/100, Train Reconstruction Loss: 7.3456, Train KL Divergence Loss: 0.2931, Train Total Loss: 7.6388, Validation Loss: 7.5841\n",
            "Epoch 4/100, Train Reconstruction Loss: 7.5613, Train KL Divergence Loss: 0.2824, Train Total Loss: 7.8437, Validation Loss: 7.7745\n",
            "Epoch 5/100, Train Reconstruction Loss: 7.2727, Train KL Divergence Loss: 0.2728, Train Total Loss: 7.5455, Validation Loss: 7.4910\n",
            "Epoch 6/100, Train Reconstruction Loss: 7.2649, Train KL Divergence Loss: 0.2638, Train Total Loss: 7.5288, Validation Loss: 7.3347\n",
            "Epoch 7/100, Train Reconstruction Loss: 7.1345, Train KL Divergence Loss: 0.2552, Train Total Loss: 7.3896, Validation Loss: 7.6009\n",
            "Epoch 8/100, Train Reconstruction Loss: 7.3656, Train KL Divergence Loss: 0.2465, Train Total Loss: 7.6121, Validation Loss: 6.8961\n",
            "Epoch 9/100, Train Reconstruction Loss: 7.2723, Train KL Divergence Loss: 0.2388, Train Total Loss: 7.5111, Validation Loss: 6.6103\n",
            "Epoch 10/100, Train Reconstruction Loss: 6.9788, Train KL Divergence Loss: 0.2317, Train Total Loss: 7.2104, Validation Loss: 6.7152\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b2018a8bccae>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Call the grid search function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-4cbd1afba8f4>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(train_loader, val_loader, hidden_sizes, latent_sizes, learning_rates, timestep, epochs, patience, dropout)\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   \u001b[0mtrain_with_early_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                   \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f1f96952e65a>\u001b[0m in \u001b[0;36mtrain_with_early_stopping\u001b[0;34m(model, train_loaders, val_loaders, optimizer, timestep, epochs, patience, beta)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}