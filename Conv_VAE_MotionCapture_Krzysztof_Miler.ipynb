{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYZ_UQmshsDm",
        "outputId": "be4b561a-50ba-455f-eaee-14e712eb630f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_from_gdrive(data_dir):\n",
        "  # Initialize an empty list to store the DataFrames\n",
        "  data_frames = []\n",
        "\n",
        "  # Loop through each .csv file in the directory\n",
        "  for dir in data_dir:\n",
        "    for file_name in os.listdir(dir):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(dir, file_name)\n",
        "\n",
        "            # Load the .csv file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Remove the first column\n",
        "            df = df.iloc[:, 4:]\n",
        "\n",
        "            df  = df.div(360)\n",
        "\n",
        "            df  = df.clip(upper =1,lower = -1)\n",
        "\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            data_frames.append(df)\n",
        "\n",
        "  # Concatenate the DataFrames into a single DataFrame\n",
        "  combined_df = pd.concat(data_frames, axis=0, ignore_index=True)\n",
        "  return combined_df"
      ],
      "metadata": {
        "id": "cCnMTybchw1o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = ['/content/drive/MyDrive/28 08 mod/Normal']\n",
        "\n",
        "combined_df =  load_from_gdrive(data_dir)\n"
      ],
      "metadata": {
        "id": "2t65JclwhyQc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class VAEAnomalyConv(nn.Module):\n",
        "    def __init__(self, input_channels, latent_size, sequence_length=59,  dropout_rate=0.0, l1_weight=0.000):\n",
        "        super(VAEAnomalyConv, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.l1_weight = l1_weight\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Flatten(start_dim=1, end_dim=-1),\n",
        "            nn.Linear(in_features=1888, out_features=latent_size)\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_size, out_features=32*sequence_length),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Unflatten(1, (32, sequence_length)),  # Adjusted this line\n",
        "            nn.ConvTranspose1d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      mu, logvar = self.encode(x)\n",
        "      #print(\"Shape after Flatten layer in encoder:\", mu.shape)\n",
        "      z = self.reparameterize(mu, logvar)\n",
        "      reconstruction = self.decode(z)\n",
        "      return reconstruction, mu, logvar\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      for i, layer in enumerate(self.encoder):\n",
        "          x = layer(x)\n",
        "          #print(f\"Shape after layer {i} in encoder:\", x.shape)\n",
        "      mu = x\n",
        "      logvar = x\n",
        "      return mu, logvar\n",
        "\n",
        "\n",
        "\n",
        "    def decode(self, z):\n",
        "        decoded = self.decoder(z)\n",
        "        return decoded\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def l1_regularization(self):\n",
        "        l1_reg = torch.tensor(0.0)\n",
        "        for param in self.parameters():\n",
        "            l1_reg += torch.norm(param, 1)\n",
        "        return self.l1_weight * l1_reg\n",
        "\n",
        "\n",
        "def train_vae(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        batch = batch.view(batch.shape[0], 1, batch.shape[1])\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logvar = model(batch)\n",
        "        loss = criterion(reconstruction, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_loader)\n",
        "\n",
        "def test_vae(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            batch = batch.view(batch.shape[0], 1, batch.shape[1])\n",
        "            reconstruction, mu, logvar = model(batch)\n",
        "            loss = criterion(reconstruction, batch, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "    return test_loss / len(test_loader)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#train_data, test_data = train_test_split(tensor_data, test_size=0.4)\n",
        "\n",
        "\n",
        "# Define the VAE model\n",
        "input_size = 59  # Number of input features\n",
        "latent_size = 64  # Size of the latent space\n",
        "batch_size = 32\n",
        "\n",
        "dropout_rate= 0.0\n",
        "\n",
        "tens_train_data = torch.tensor(combined_df.values, dtype=torch.float32)\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(tens_train_data.unsqueeze(2), batch_size=batch_size, shuffle=True)\n",
        "#test_loader = DataLoader(test_data.unsqueeze(2), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vae = VAEAnomalyConv(input_size, latent_size).to(device)\n",
        "print(vae)\n",
        "\n",
        "# Define the loss function\n",
        "def vae_loss(reconstruction, x, mu, logvar):\n",
        "    recon_loss = nn.functional.mse_loss(reconstruction, x, reduction='sum')\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_divergence\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(vae.parameters(), lr=0.001)  # Adjust learning rate\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_vae(vae, train_loader, optimizer, vae_loss, device)\n",
        "    #test_loss = test_vae(vae, test_loader, vae_loss, device)\n",
        "    print(f'Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "D0b0mSMLh0DN",
        "outputId": "bec9a752-cc16-4615-e851-b662010859d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAEAnomalyConv(\n",
            "  (encoder): Sequential(\n",
            "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.0, inplace=False)\n",
            "    (3): Flatten(start_dim=1, end_dim=-1)\n",
            "    (4): Linear(in_features=1888, out_features=64, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=1888, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.0, inplace=False)\n",
            "    (3): Unflatten(dim=1, unflattened_size=(32, 59))\n",
            "    (4): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-49ee75ef6b0f>\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;31m#test_loss = test_vae(vae, test_loader, vae_loss, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-49ee75ef6b0f>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-49ee75ef6b0f>\u001b[0m in \u001b[0;36mvae_loss\u001b[0;34m(reconstruction, x, mu, logvar)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Define the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mkl_divergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class VAEAnomalyConv(nn.Module):\n",
        "    def __init__(self, input_channels, latent_size, num_filters, sequence_length=59,  dropout_rate=0.0, l1_weight=0.000):\n",
        "        super(VAEAnomalyConv, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.l1_weight = l1_weight\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Flatten(start_dim=1, end_dim=-1),\n",
        "            nn.Linear(in_features=num_filters*sequence_length, out_features=latent_size)\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_size, out_features=num_filters*sequence_length),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Unflatten(1, (num_filters, sequence_length)),\n",
        "            nn.ConvTranspose1d(in_channels=num_filters, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      mu, logvar = self.encode(x)\n",
        "      #print(\"Shape after Flatten layer in encoder:\", mu.shape)\n",
        "      z = self.reparameterize(mu, logvar)\n",
        "      reconstruction = self.decode(z)\n",
        "      return reconstruction, mu, logvar\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      for i, layer in enumerate(self.encoder):\n",
        "          x = layer(x)\n",
        "          #print(f\"Shape after layer {i} in encoder:\", x.shape)\n",
        "      mu = x\n",
        "      logvar = x\n",
        "      return mu, logvar\n",
        "\n",
        "\n",
        "\n",
        "    def decode(self, z):\n",
        "        decoded = self.decoder(z)\n",
        "        return decoded\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def l1_regularization(self):\n",
        "        l1_reg = torch.tensor(0.0)\n",
        "        for param in self.parameters():\n",
        "            l1_reg += torch.norm(param, 1)\n",
        "        return self.l1_weight * l1_reg\n",
        "\n",
        "\n",
        "def train_vae(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        batch = batch.view(batch.shape[0], 1, batch.shape[1])\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logvar = model(batch)\n",
        "        loss = criterion(reconstruction, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_loader)\n",
        "\n",
        "def test_vae(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            batch = batch.view(batch.shape[0], 1, batch.shape[1])\n",
        "            reconstruction, mu, logvar = model(batch)\n",
        "            loss = criterion(reconstruction, batch, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "    return test_loss / len(test_loader)\n",
        "\n",
        "# Define the loss function\n",
        "def vae_loss(reconstruction, x, mu, logvar):\n",
        "    recon_loss = nn.functional.mse_loss(reconstruction, x, reduction='sum')\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_divergence\n",
        "\n",
        "input_size = 59\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.01],\n",
        "    'dropout_rate': [0.0, 0.2],\n",
        "    'latent_size': [128, 64],\n",
        "    'num_filters': [32, 64],\n",
        "}\n",
        "#change kernel size to 3/7 and padding to 1/3 by hand, leave stride to 1\n",
        "log_df = pd.DataFrame(columns=['epoch', 'train_loss', 'test_loss', 'parameters'])\n",
        "\n",
        "# Define the directory to save the model and log\n",
        "save_dir = '/content/drive/MyDrive/MGR_VAE_CNN/'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "\n",
        "tensor_data = torch.tensor(combined_df.values, dtype=torch.float32)\n",
        "train_data, test_data = train_test_split(tensor_data, test_size=0.4,random_state = 42)\n",
        "validation_data, test_data = train_test_split(test_data, test_size=0.5,random_state = 42)\n",
        "\n",
        "# Grid search\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f'Current parameters: {params}')\n",
        "    # Update params\n",
        "    vae = VAEAnomalyConv(input_size, params['latent_size'], params['num_filters'], dropout_rate=params['dropout_rate']).to(device)\n",
        "    train_loader = DataLoader(train_data.unsqueeze(2), batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_data.unsqueeze(2), batch_size=32, shuffle=False)\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=params['lr'])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(50):\n",
        "        train_loss = train_vae(vae, train_loader, optimizer, vae_loss, device)\n",
        "        test_loss = test_vae(vae, test_loader, vae_loss, device)\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{50}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if epoch > 5 and log_df['test_loss'].iloc[-5:].mean() < test_loss:\n",
        "            print(\"Break\")\n",
        "            break\n",
        "\n",
        "    # Save model and training log\n",
        "    model_path = f'{save_dir}model_{test_loss:.4f}_{params}.pt'\n",
        "    log_path = f'{save_dir}log_{test_loss:.4f}_{params}.csv'\n",
        "    torch.save(vae.state_dict(), model_path)\n",
        "    log_df.to_csv(log_path, index=False)\n",
        "\n",
        "    # Print the names of the saved files\n",
        "    print(f'Saved model to {model_path}')\n",
        "    print(f'Saved log to {log_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Sgw09GD6iHxj",
        "outputId": "8d9aabac-add2-42f7-b120-cfe74dfeb829"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current parameters: {'dropout_rate': 0.0, 'latent_size': 128, 'lr': 0.001, 'num_filters': 32}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-654209c7f07b>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Train the model for a fixed number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-654209c7f07b>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 state_steps)\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    312\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}